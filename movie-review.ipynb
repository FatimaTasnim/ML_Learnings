{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttextcrawl300d2m', 'movie-review-sentiment-analysis-kernels-only']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import datetime\n",
    "import lightgbm as lgb\n",
    "from scipy import stats\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "pd.set_option('max_colwidth',400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "570249da8b1fe9380b299676e772ef2ff5896306"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/train.tsv',delimiter='\\t',encoding='utf-8')\n",
    "test = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/test.tsv',delimiter='\\t',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "c769bc49b7834ab4e3e0234ad74d9f6868b5203e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage that what is good for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>of escapades demonstrating the adage that what is good for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>escapades demonstrating the adage that what is good for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>escapades</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>demonstrating the adage that what is good for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId    ...      Sentiment\n",
       "0         1    ...              1\n",
       "1         2    ...              2\n",
       "2         3    ...              2\n",
       "3         4    ...              2\n",
       "4         5    ...              2\n",
       "5         6    ...              2\n",
       "6         7    ...              2\n",
       "7         8    ...              2\n",
       "8         9    ...              2\n",
       "9        10    ...              2\n",
       "\n",
       "[10 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "79c4d60a75b706e4960f1d3ceb97aad4c62d3622"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>This quiet , introspective and entertaining independent is worth seeking .</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "      <td>This quiet , introspective and entertaining independent</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>66</td>\n",
       "      <td>2</td>\n",
       "      <td>This</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>67</td>\n",
       "      <td>2</td>\n",
       "      <td>quiet , introspective and entertaining independent</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>quiet , introspective and entertaining</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>quiet</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>, introspective and entertaining</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>71</td>\n",
       "      <td>2</td>\n",
       "      <td>introspective and entertaining</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>72</td>\n",
       "      <td>2</td>\n",
       "      <td>introspective and</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>73</td>\n",
       "      <td>2</td>\n",
       "      <td>introspective</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>and</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>75</td>\n",
       "      <td>2</td>\n",
       "      <td>entertaining</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>76</td>\n",
       "      <td>2</td>\n",
       "      <td>independent</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>77</td>\n",
       "      <td>2</td>\n",
       "      <td>is worth seeking .</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>78</td>\n",
       "      <td>2</td>\n",
       "      <td>is worth seeking</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>79</td>\n",
       "      <td>2</td>\n",
       "      <td>is worth</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>worth</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>81</td>\n",
       "      <td>2</td>\n",
       "      <td>seeking</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    PhraseId    ...      Sentiment\n",
       "63        64    ...              4\n",
       "64        65    ...              3\n",
       "65        66    ...              2\n",
       "66        67    ...              4\n",
       "67        68    ...              3\n",
       "68        69    ...              2\n",
       "69        70    ...              3\n",
       "70        71    ...              3\n",
       "71        72    ...              3\n",
       "72        73    ...              2\n",
       "73        74    ...              2\n",
       "74        75    ...              4\n",
       "75        76    ...              2\n",
       "76        77    ...              3\n",
       "77        78    ...              4\n",
       "78        79    ...              2\n",
       "79        80    ...              2\n",
       "80        81    ...              2\n",
       "\n",
       "[18 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[train.SentenceId == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "db6a953bd7bcf6f30f5b12aa393bbc51ef49b03f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average count of phrases per sentence in train is 18.\n",
      "Average count of phrases per sentence in test is 20.\n"
     ]
    }
   ],
   "source": [
    "print('Average count of phrases per sentence in train is {0:.0f}.'.format(train.groupby('SentenceId')['Phrase'].count().mean()))\n",
    "print('Average count of phrases per sentence in test is {0:.0f}.'.format(test.groupby('SentenceId')['Phrase'].count().mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "fecf16d2f10ada19ebbf9e3b51d2bcd3661ec0d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of phrases in train: 156060. Number of sentences in train: 8529.\n",
      "Number of phrases in test: 66292. Number of sentences in test: 3310.\n"
     ]
    }
   ],
   "source": [
    "print('Number of phrases in train: {}. Number of sentences in train: {}.'.format(train.shape[0], len(train.SentenceId.unique())))\n",
    "print('Number of phrases in test: {}. Number of sentences in test: {}.'.format(test.shape[0], len(test.SentenceId.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "5ef36b96b27b405116981e0d741a6a4a09898928"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length of phrases in train is 7.\n",
      "Average word length of phrases in test is 7.\n"
     ]
    }
   ],
   "source": [
    "print('Average word length of phrases in train is {0:.0f}.'.format(np.mean(train['Phrase'].apply(lambda x: len(x.split())))))\n",
    "print('Average word length of phrases in test is {0:.0f}.'.format(np.mean(test['Phrase'].apply(lambda x: len(x.split())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "5b356b7f75fd0cd79ecd03103d563aee97c67ebe"
   },
   "outputs": [],
   "source": [
    "text = ' '.join(train.loc[train.Sentiment == 4, 'Phrase'].values)\n",
    "text_trigrams = [i for i in ngrams(text.split(), 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "23a0b7a9d6f8460cae333a4dd217561af0d744c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('one', 'of', 'the'), 199),\n",
       " (('of', 'the', 'year'), 103),\n",
       " (('.', 'is', 'a'), 87),\n",
       " (('of', 'the', 'best'), 80),\n",
       " (('of', 'the', 'most'), 70),\n",
       " (('is', 'one', 'of'), 50),\n",
       " (('One', 'of', 'the'), 43),\n",
       " ((',', 'and', 'the'), 40),\n",
       " (('the', 'year', \"'s\"), 38),\n",
       " (('It', \"'s\", 'a'), 38),\n",
       " (('it', \"'s\", 'a'), 37),\n",
       " (('.', \"'s\", 'a'), 37),\n",
       " (('a', 'movie', 'that'), 35),\n",
       " (('the', 'edge', 'of'), 34),\n",
       " (('the', 'kind', 'of'), 33),\n",
       " (('of', 'your', 'seat'), 33),\n",
       " (('the', 'film', 'is'), 31),\n",
       " ((',', 'this', 'is'), 31),\n",
       " (('the', 'film', \"'s\"), 31),\n",
       " ((',', 'the', 'film'), 30),\n",
       " (('film', 'that', 'is'), 30),\n",
       " (('as', 'one', 'of'), 30),\n",
       " (('edge', 'of', 'your'), 29),\n",
       " ((',', 'it', \"'s\"), 27),\n",
       " (('a', 'film', 'that'), 27),\n",
       " (('as', 'well', 'as'), 27),\n",
       " ((',', 'funny', ','), 25),\n",
       " ((',', 'but', 'it'), 23),\n",
       " (('films', 'of', 'the'), 23),\n",
       " (('some', 'of', 'the'), 23)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(text_trigrams).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "626abc5cf0bc04bdcb04a8d5c2c2abd8872dcf40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((',', 'funny', ','), 33),\n",
       " (('one', 'year', \"'s\"), 28),\n",
       " (('year', \"'s\", 'best'), 26),\n",
       " (('movies', 'ever', 'made'), 19),\n",
       " ((',', 'solid', 'cast'), 19),\n",
       " (('solid', 'cast', ','), 18),\n",
       " ((\"'ve\", 'ever', 'seen'), 16),\n",
       " (('.', 'It', \"'s\"), 16),\n",
       " ((',', 'making', 'one'), 15),\n",
       " (('best', 'films', 'year'), 15),\n",
       " ((',', 'touching', ','), 15),\n",
       " (('exquisite', 'acting', ','), 15),\n",
       " (('acting', ',', 'inventive'), 14),\n",
       " ((',', 'inventive', 'screenplay'), 14),\n",
       " (('jaw-dropping', 'action', 'sequences'), 14),\n",
       " (('good', 'acting', ','), 14),\n",
       " ((\"'s\", 'best', 'films'), 14),\n",
       " (('I', \"'ve\", 'seen'), 14),\n",
       " (('funny', ',', 'even'), 14),\n",
       " (('best', 'war', 'movies'), 13),\n",
       " (('purely', 'enjoyable', 'satisfying'), 13),\n",
       " (('funny', ',', 'touching'), 13),\n",
       " ((',', 'smart', ','), 13),\n",
       " (('inventive', 'screenplay', ','), 13),\n",
       " (('funniest', 'jokes', 'movie'), 13),\n",
       " (('action', 'sequences', ','), 13),\n",
       " (('sequences', ',', 'striking'), 13),\n",
       " ((',', 'striking', 'villains'), 13),\n",
       " (('exquisite', 'motion', 'picture'), 13),\n",
       " (('war', 'movies', 'ever'), 12)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(train.loc[train.Sentiment == 4, 'Phrase'].values)\n",
    "text = [i for i in text.split() if i not in stopwords.words('english')]\n",
    "text_trigrams = [i for i in ngrams(text, 3)]\n",
    "Counter(text_trigrams).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "dc5ad9dc3d0624212a0f18a7af5ae0a56c88cb4e"
   },
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "214cdd6ab00b4c7d4f9b9150169e22e24de71a80"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), tokenizer=tokenizer.tokenize)\n",
    "full_text = list(train['Phrase'].values) + list(test['Phrase'].values)\n",
    "vectorizer.fit(full_text)\n",
    "train_vectorized = vectorizer.transform(train['Phrase'])\n",
    "test_vectorized = vectorizer.transform(test['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "67da69ed129aff55b5a0fac1e3abd0f2f7f88183"
   },
   "outputs": [],
   "source": [
    "y = train['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "b591deeca045d80c312f9e229c183307ace23242"
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "ovr = OneVsRestClassifier(logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "d64360fc92dc880208bd2c09b0b9e3cbe1ef47c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.77 s, sys: 20 ms, total: 8.79 s\n",
      "Wall time: 8.81 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "          n_jobs=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ovr.fit(train_vectorized, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "719f0ebadfdd451c6f0008eb682ce16be87497c2"
   },
   "source": [
    "#### n_jobs : int or None, optional (default=None)\n",
    "The number of CPUs to use to do the computation. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details.\n",
    "#### cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are:\n",
    "\n",
    "None, to use the default 3-fold cross validation,\n",
    "integer, to specify the number of folds in a (Stratified)KFold,\n",
    "CV splitter,\n",
    "An iterable yielding (train, test) splits as arrays of indices.\n",
    "For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used. In all other cases, KFold is used.\n",
    "\n",
    "Refer User Guide for the various cross-validation strategies that can be used her"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "db46b73a716f906b28b81ebd66bbb0a5354905ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation mean accuracy 56.55%, std 0.07.\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(ovr, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=3)\n",
    "print('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "6ea21c878d7d12c51f47f33057f470509b7ba736"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation mean accuracy 56.51%, std 0.68.\n",
      "CPU times: user 60 ms, sys: 36 ms, total: 96 ms\n",
      "Wall time: 25.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svc = LinearSVC(dual=False)\n",
    "scores = cross_val_score(svc, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=3)\n",
    "print('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "455307e6524e5a5da8bc71dfe4e0180c3720e8b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "ovr.fit(train_vectorized, y);\n",
    "svc.fit(train_vectorized, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "036388eea6f8418e6b8336462182be371c5cb2d6"
   },
   "source": [
    "## Deep learning\n",
    "And now let's try DL. DL should work better for text classification with multiple layers. I use an architecture similar to those which were used in toxic competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "f0acb9c77834ccfcac81eb5b6b840a89a7e8fecd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras.models import Model, load_model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras import backend as K\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b14708b1ea0fc549c552e07ae4643c3f31e70403"
   },
   "source": [
    "#### t.fit_texts()\n",
    "The tokenizer provides:\n",
    "- word counts\n",
    "- word documensts\n",
    "- word index\n",
    "- document count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c6c5204cadc8dcec14bd67c0573ffe7bccc4b275"
   },
   "source": [
    "#### lower: boolean. Whether to convert the texts to lowercase.\n",
    "#### filters: a string where each element is a character that will be filtered from the texts. The default is all punctuation, plus tabs and line breaks, minus the ' character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "168928f78760dd07bf0146868675f99f2cc417c3"
   },
   "outputs": [],
   "source": [
    "tk = Tokenizer(lower = True, filters='')\n",
    "tk.fit_on_texts(full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3d1cd746fdf18635ae1203cbc07262f75dbcef52"
   },
   "source": [
    "#### texts_to_sequences()\n",
    "only top \"num_words\" most frequent words will be taken into account. Only word known by the tokenizer will  be taken into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "379a9edeaf2067672b86cdec12e95e78c490a10c"
   },
   "outputs": [],
   "source": [
    "train_tokenized = tk.texts_to_sequences(train['Phrase'])\n",
    "test_tokenized = tk.texts_to_sequences(test['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "a1816a465379621b7345088b5f1145103f0bfb88"
   },
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "X_train = pad_sequences(train_tokenized, maxlen = max_len)\n",
    "X_test = pad_sequences(test_tokenized, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "5bfc9fe49271e8ad61571af7a2f2181e173eafe0"
   },
   "outputs": [],
   "source": [
    "embedding_path = \"../input/fasttextcrawl300d2m/crawl-300d-2M_1.vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "da7cddd1e6c4ed8bbf5604433e10e78640f838cc"
   },
   "outputs": [],
   "source": [
    "embed_size = 300\n",
    "max_features = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "4379dbf4df91aba284554427fd138050a597e59e"
   },
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n",
    "\n",
    "word_index = tk.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words + 1, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "89862f4f8fa8fc58aa7ddae77900ced50dcac2ca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "y_ohe = ohe.fit_transform(y.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dee9848425a9832d7520b7597f00d821073e451b"
   },
   "source": [
    "#### ModelCheckpoint:\n",
    "Save the model after every epoch.\n",
    "#### lr: float >= 0. Learning rate.\n",
    "#### decay: float >= 0. Learning rate decay over each update.\n",
    "#### units: Positive integer, dimensionality of the output space.\n",
    "#### filters/conv_size: Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "5d2c9fc921c74d50e28dbcd6865b74bcb099ef79"
   },
   "outputs": [],
   "source": [
    "def build_model1(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n",
    "    file_path = \"best_model.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "                                  save_best_only = True, mode = \"min\")\n",
    "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
    "    \n",
    "    inp = Input(shape = (max_len,))\n",
    "    x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
    "\n",
    "    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n",
    "    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool1_gru = GlobalAveragePooling1D()(x1)\n",
    "    max_pool1_gru = GlobalMaxPooling1D()(x1)\n",
    "    \n",
    "    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool3_gru = GlobalAveragePooling1D()(x3)\n",
    "    max_pool3_gru = GlobalMaxPooling1D()(x3)\n",
    "    \n",
    "    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n",
    "    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool1_lstm = GlobalAveragePooling1D()(x1)\n",
    "    max_pool1_lstm = GlobalMaxPooling1D()(x1)\n",
    "    \n",
    "    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool3_lstm = GlobalAveragePooling1D()(x3)\n",
    "    max_pool3_lstm = GlobalMaxPooling1D()(x3)\n",
    "    \n",
    "    \n",
    "    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool3_gru, max_pool3_gru,\n",
    "                    avg_pool1_lstm, max_pool1_lstm, avg_pool3_lstm, max_pool3_lstm])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
    "    x = Dense(5, activation = \"sigmoid\")(x)\n",
    "    model = Model(inputs = inp, outputs = x)\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.1, \n",
    "                        verbose = 1, callbacks = [check_point, early_stop])\n",
    "    model = load_model(file_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "518d4c9473ee2ebb24cfa70b9b261808fb77a436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140454 samples, validate on 15606 samples\n",
      "Epoch 1/20\n",
      "140454/140454 [==============================] - 30s 214us/step - loss: 0.5759 - acc: 0.8048 - val_loss: 0.5014 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.50142, saving model to best_model.hdf5\n",
      "Epoch 2/20\n",
      "140454/140454 [==============================] - 25s 179us/step - loss: 0.4627 - acc: 0.8050 - val_loss: 0.4469 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.50142 to 0.44690, saving model to best_model.hdf5\n",
      "Epoch 3/20\n",
      "140454/140454 [==============================] - 25s 179us/step - loss: 0.4296 - acc: 0.8050 - val_loss: 0.4306 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.44690 to 0.43062, saving model to best_model.hdf5\n",
      "Epoch 4/20\n",
      "140454/140454 [==============================] - 26s 183us/step - loss: 0.4192 - acc: 0.8050 - val_loss: 0.4254 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.43062 to 0.42543, saving model to best_model.hdf5\n",
      "Epoch 5/20\n",
      "140454/140454 [==============================] - 25s 178us/step - loss: 0.4160 - acc: 0.8050 - val_loss: 0.4240 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.42543 to 0.42398, saving model to best_model.hdf5\n",
      "Epoch 6/20\n",
      "140454/140454 [==============================] - 25s 178us/step - loss: 0.4153 - acc: 0.8050 - val_loss: 0.4239 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.42398 to 0.42387, saving model to best_model.hdf5\n",
      "Epoch 7/20\n",
      "140454/140454 [==============================] - 26s 185us/step - loss: 0.4152 - acc: 0.8050 - val_loss: 0.4238 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.42387 to 0.42384, saving model to best_model.hdf5\n",
      "Epoch 8/20\n",
      "140454/140454 [==============================] - 25s 177us/step - loss: 0.4152 - acc: 0.8050 - val_loss: 0.4238 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.42384 to 0.42382, saving model to best_model.hdf5\n",
      "Epoch 9/20\n",
      "140454/140454 [==============================] - 25s 177us/step - loss: 0.4152 - acc: 0.8050 - val_loss: 0.4238 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.42382\n",
      "Epoch 10/20\n",
      "140454/140454 [==============================] - 26s 187us/step - loss: 0.4152 - acc: 0.8050 - val_loss: 0.4240 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.42382\n",
      "Epoch 11/20\n",
      "140454/140454 [==============================] - 26s 182us/step - loss: 0.4152 - acc: 0.8050 - val_loss: 0.4240 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.42382\n"
     ]
    }
   ],
   "source": [
    "model1 = build_model1(lr = 1e-3, lr_d = 1e-10, units = 64, spatial_dr = 0.3, kernel_size1=3, kernel_size2=2, dense_units=32, dr=0.1, conv_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "9b87a8c5a789334c343905a2b863a605dccbe662"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140454 samples, validate on 15606 samples\n",
      "Epoch 1/20\n",
      "140454/140454 [==============================] - 31s 224us/step - loss: 0.5758 - acc: 0.8049 - val_loss: 0.5014 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.50136, saving model to best_model.hdf5\n",
      "Epoch 2/20\n",
      "140454/140454 [==============================] - 28s 200us/step - loss: 0.4627 - acc: 0.8050 - val_loss: 0.4469 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.50136 to 0.44691, saving model to best_model.hdf5\n",
      "Epoch 3/20\n",
      "140454/140454 [==============================] - 28s 199us/step - loss: 0.4296 - acc: 0.8050 - val_loss: 0.4306 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.44691 to 0.43058, saving model to best_model.hdf5\n",
      "Epoch 4/20\n",
      "140454/140454 [==============================] - 28s 200us/step - loss: 0.4192 - acc: 0.8050 - val_loss: 0.4254 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.43058 to 0.42536, saving model to best_model.hdf5\n",
      "Epoch 5/20\n",
      "140454/140454 [==============================] - 28s 199us/step - loss: 0.4160 - acc: 0.8050 - val_loss: 0.4241 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.42536 to 0.42405, saving model to best_model.hdf5\n",
      "Epoch 6/20\n",
      "140454/140454 [==============================] - 28s 199us/step - loss: 0.4153 - acc: 0.8050 - val_loss: 0.4238 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.42405 to 0.42382, saving model to best_model.hdf5\n",
      "Epoch 7/20\n",
      "140454/140454 [==============================] - 28s 200us/step - loss: 0.4152 - acc: 0.8050 - val_loss: 0.4239 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.42382\n",
      "Epoch 8/20\n",
      "140454/140454 [==============================] - 28s 199us/step - loss: 0.4152 - acc: 0.8050 - val_loss: 0.4239 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.42382\n",
      "Epoch 9/20\n",
      "140454/140454 [==============================] - 28s 199us/step - loss: 0.4152 - acc: 0.8050 - val_loss: 0.4239 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.42382\n"
     ]
    }
   ],
   "source": [
    "model2 = build_model1(lr = 1e-3, lr_d = 1e-10, units = 128, spatial_dr = 0.5, kernel_size1=3, kernel_size2=2, dense_units=64, dr=0.2, conv_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "1a4fdaff0f14bd451566e7ed495894ea7b76667d"
   },
   "outputs": [],
   "source": [
    "def build_model2(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n",
    "    file_path = \"best_model.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "                                  save_best_only = True, mode = \"min\")\n",
    "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
    "\n",
    "    inp = Input(shape = (max_len,))\n",
    "    x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
    "\n",
    "    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n",
    "    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n",
    "    \n",
    "    x_conv1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool1_gru = GlobalAveragePooling1D()(x_conv1)\n",
    "    max_pool1_gru = GlobalMaxPooling1D()(x_conv1)\n",
    "    \n",
    "    x_conv2 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool2_gru = GlobalAveragePooling1D()(x_conv2)\n",
    "    max_pool2_gru = GlobalMaxPooling1D()(x_conv2)\n",
    "    \n",
    "    \n",
    "    x_conv3 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool1_lstm = GlobalAveragePooling1D()(x_conv3)\n",
    "    max_pool1_lstm = GlobalMaxPooling1D()(x_conv3)\n",
    "    \n",
    "    x_conv4 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool2_lstm = GlobalAveragePooling1D()(x_conv4)\n",
    "    max_pool2_lstm = GlobalMaxPooling1D()(x_conv4)\n",
    "    \n",
    "    \n",
    "    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool2_gru, max_pool2_gru,\n",
    "                    avg_pool1_lstm, max_pool1_lstm, avg_pool2_lstm, max_pool2_lstm])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
    "    x = Dense(5, activation = \"sigmoid\")(x)\n",
    "    model = Model(inputs = inp, outputs = x)\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.1, \n",
    "                        verbose = 1, callbacks = [check_point, early_stop])\n",
    "    model = load_model(file_path)\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "0feee2a2fd9d47c16cd46a25153251d5687f8974"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140454 samples, validate on 15606 samples\n",
      "Epoch 1/20\n",
      "140454/140454 [==============================] - 30s 211us/step - loss: 0.6771 - acc: 0.8049 - val_loss: 0.6623 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.66229, saving model to best_model.hdf5\n",
      "Epoch 2/20\n",
      "140454/140454 [==============================] - 25s 175us/step - loss: 0.6472 - acc: 0.8050 - val_loss: 0.6345 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.66229 to 0.63445, saving model to best_model.hdf5\n",
      "Epoch 3/20\n",
      "140454/140454 [==============================] - 26s 189us/step - loss: 0.6202 - acc: 0.8050 - val_loss: 0.6092 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63445 to 0.60920, saving model to best_model.hdf5\n",
      "Epoch 4/20\n",
      "140454/140454 [==============================] - 25s 175us/step - loss: 0.5956 - acc: 0.8050 - val_loss: 0.5863 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.60920 to 0.58628, saving model to best_model.hdf5\n",
      "Epoch 5/20\n",
      "140454/140454 [==============================] - 25s 175us/step - loss: 0.5733 - acc: 0.8050 - val_loss: 0.5655 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.58628 to 0.56551, saving model to best_model.hdf5\n",
      "Epoch 6/20\n",
      "140454/140454 [==============================] - 25s 179us/step - loss: 0.5530 - acc: 0.8050 - val_loss: 0.5468 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.56551 to 0.54677, saving model to best_model.hdf5\n",
      "Epoch 7/20\n",
      "140454/140454 [==============================] - 25s 179us/step - loss: 0.5348 - acc: 0.8050 - val_loss: 0.5300 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.54677 to 0.52995, saving model to best_model.hdf5\n",
      "Epoch 8/20\n",
      "140454/140454 [==============================] - 25s 175us/step - loss: 0.5184 - acc: 0.8050 - val_loss: 0.5150 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.52995 to 0.51495, saving model to best_model.hdf5\n",
      "Epoch 9/20\n",
      "140454/140454 [==============================] - 25s 175us/step - loss: 0.5038 - acc: 0.8050 - val_loss: 0.5017 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.51495 to 0.50168, saving model to best_model.hdf5\n",
      "Epoch 10/20\n",
      "140454/140454 [==============================] - 26s 183us/step - loss: 0.4909 - acc: 0.8050 - val_loss: 0.4900 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.50168 to 0.49002, saving model to best_model.hdf5\n",
      "Epoch 11/20\n",
      "140454/140454 [==============================] - 25s 175us/step - loss: 0.4795 - acc: 0.8050 - val_loss: 0.4799 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.49002 to 0.47988, saving model to best_model.hdf5\n",
      "Epoch 12/20\n",
      "140454/140454 [==============================] - 25s 175us/step - loss: 0.4697 - acc: 0.8050 - val_loss: 0.4711 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.47988 to 0.47113, saving model to best_model.hdf5\n",
      "Epoch 13/20\n",
      "140454/140454 [==============================] - 26s 184us/step - loss: 0.4611 - acc: 0.8050 - val_loss: 0.4636 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.47113 to 0.46362, saving model to best_model.hdf5\n",
      "Epoch 14/20\n",
      "140454/140454 [==============================] - 26s 183us/step - loss: 0.4538 - acc: 0.8050 - val_loss: 0.4572 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.46362 to 0.45722, saving model to best_model.hdf5\n",
      "Epoch 15/20\n",
      "140454/140454 [==============================] - 25s 181us/step - loss: 0.4475 - acc: 0.8050 - val_loss: 0.4518 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.45722 to 0.45176, saving model to best_model.hdf5\n",
      "Epoch 16/20\n",
      "140454/140454 [==============================] - 26s 183us/step - loss: 0.4421 - acc: 0.8050 - val_loss: 0.4471 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.45176 to 0.44711, saving model to best_model.hdf5\n",
      "Epoch 17/20\n",
      "140454/140454 [==============================] - 25s 175us/step - loss: 0.4375 - acc: 0.8050 - val_loss: 0.4431 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.44711 to 0.44310, saving model to best_model.hdf5\n",
      "Epoch 18/20\n",
      "140454/140454 [==============================] - 25s 175us/step - loss: 0.4335 - acc: 0.8050 - val_loss: 0.4396 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.44310 to 0.43963, saving model to best_model.hdf5\n",
      "Epoch 19/20\n",
      "140454/140454 [==============================] - 26s 183us/step - loss: 0.4301 - acc: 0.8050 - val_loss: 0.4366 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.43963 to 0.43664, saving model to best_model.hdf5\n",
      "Epoch 20/20\n",
      "140454/140454 [==============================] - 25s 175us/step - loss: 0.4272 - acc: 0.8050 - val_loss: 0.4341 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.43664 to 0.43406, saving model to best_model.hdf5\n"
     ]
    }
   ],
   "source": [
    "model3 = build_model2(lr = 1e-4, lr_d = 0, units = 64, spatial_dr = 0.5, kernel_size1=4, kernel_size2=3, dense_units=32, dr=0.1, conv_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "6df39de432402be72114f66cc3f7e27d8579074c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140454 samples, validate on 15606 samples\n",
      "Epoch 1/20\n",
      "140454/140454 [==============================] - 31s 217us/step - loss: 0.5757 - acc: 0.8049 - val_loss: 0.5013 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.50134, saving model to best_model.hdf5\n",
      "Epoch 2/20\n",
      "140454/140454 [==============================] - 25s 175us/step - loss: 0.4627 - acc: 0.8050 - val_loss: 0.4469 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.50134 to 0.44687, saving model to best_model.hdf5\n",
      "Epoch 3/20\n",
      "140454/140454 [==============================] - 25s 175us/step - loss: 0.4296 - acc: 0.8050 - val_loss: 0.4307 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.44687 to 0.43066, saving model to best_model.hdf5\n",
      "Epoch 4/20\n",
      "140454/140454 [==============================] - 25s 180us/step - loss: 0.4192 - acc: 0.8050 - val_loss: 0.4254 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.43066 to 0.42536, saving model to best_model.hdf5\n",
      "Epoch 5/20\n",
      "140454/140454 [==============================] - 26s 188us/step - loss: 0.4160 - acc: 0.8050 - val_loss: 0.4241 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.42536 to 0.42409, saving model to best_model.hdf5\n",
      "Epoch 6/20\n",
      "140454/140454 [==============================] - 25s 181us/step - loss: 0.4153 - acc: 0.8050 - val_loss: 0.4238 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.42409 to 0.42380, saving model to best_model.hdf5\n",
      "Epoch 7/20\n",
      "140454/140454 [==============================] - 25s 177us/step - loss: 0.4152 - acc: 0.8050 - val_loss: 0.4239 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.42380\n",
      "Epoch 8/20\n",
      "140454/140454 [==============================] - 26s 182us/step - loss: 0.4152 - acc: 0.8050 - val_loss: 0.4237 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.42380 to 0.42374, saving model to best_model.hdf5\n",
      "Epoch 9/20\n",
      "140454/140454 [==============================] - 25s 175us/step - loss: 0.4152 - acc: 0.8050 - val_loss: 0.4239 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.42374\n",
      "Epoch 10/20\n",
      "140454/140454 [==============================] - 25s 175us/step - loss: 0.4152 - acc: 0.8050 - val_loss: 0.4237 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.42374 to 0.42369, saving model to best_model.hdf5\n",
      "Epoch 11/20\n",
      "140454/140454 [==============================] - 26s 184us/step - loss: 0.4152 - acc: 0.8050 - val_loss: 0.4238 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.42369\n",
      "Epoch 12/20\n",
      "140454/140454 [==============================] - 25s 175us/step - loss: 0.4152 - acc: 0.8050 - val_loss: 0.4238 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.42369\n",
      "Epoch 13/20\n",
      "140454/140454 [==============================] - 25s 175us/step - loss: 0.4152 - acc: 0.8050 - val_loss: 0.4240 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.42369\n"
     ]
    }
   ],
   "source": [
    "model4 = build_model2(lr = 1e-3, lr_d = 0, units = 64, spatial_dr = 0.5, kernel_size1=3, kernel_size2=3, dense_units=64, dr=0.3, conv_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "a20f58ff18e93b420117aa599be15496d8f7bdc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140454 samples, validate on 15606 samples\n",
      "Epoch 1/20\n",
      "140454/140454 [==============================] - 34s 241us/step - loss: 0.5758 - acc: 0.8049 - val_loss: 0.5014 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.50137, saving model to best_model.hdf5\n",
      "Epoch 2/20\n",
      "140454/140454 [==============================] - 27s 190us/step - loss: 0.4626 - acc: 0.8050 - val_loss: 0.4469 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.50137 to 0.44692, saving model to best_model.hdf5\n",
      "Epoch 3/20\n",
      "140454/140454 [==============================] - 27s 195us/step - loss: 0.4296 - acc: 0.8050 - val_loss: 0.4307 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.44692 to 0.43066, saving model to best_model.hdf5\n",
      "Epoch 4/20\n",
      "140454/140454 [==============================] - 26s 187us/step - loss: 0.4192 - acc: 0.8050 - val_loss: 0.4255 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.43066 to 0.42547, saving model to best_model.hdf5\n",
      "Epoch 5/20\n",
      "140454/140454 [==============================] - 26s 188us/step - loss: 0.4160 - acc: 0.8050 - val_loss: 0.4239 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.42547 to 0.42390, saving model to best_model.hdf5\n",
      "Epoch 6/20\n",
      "140454/140454 [==============================] - 27s 190us/step - loss: 0.4153 - acc: 0.8050 - val_loss: 0.4239 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.42390 to 0.42387, saving model to best_model.hdf5\n",
      "Epoch 7/20\n",
      "140454/140454 [==============================] - 26s 187us/step - loss: 0.4152 - acc: 0.8050 - val_loss: 0.4239 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.42387 to 0.42385, saving model to best_model.hdf5\n",
      "Epoch 8/20\n",
      "140454/140454 [==============================] - 26s 188us/step - loss: 0.4152 - acc: 0.8050 - val_loss: 0.4239 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.42385 to 0.42385, saving model to best_model.hdf5\n",
      "Epoch 9/20\n",
      "140454/140454 [==============================] - 27s 191us/step - loss: 0.4152 - acc: 0.8050 - val_loss: 0.4238 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.42385 to 0.42382, saving model to best_model.hdf5\n",
      "Epoch 10/20\n",
      "140454/140454 [==============================] - 26s 187us/step - loss: 0.4152 - acc: 0.8050 - val_loss: 0.4239 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.42382\n",
      "Epoch 11/20\n",
      "140454/140454 [==============================] - 26s 188us/step - loss: 0.4152 - acc: 0.8050 - val_loss: 0.4239 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.42382\n",
      "Epoch 12/20\n",
      "140454/140454 [==============================] - 27s 191us/step - loss: 0.4152 - acc: 0.8050 - val_loss: 0.4239 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.42382\n"
     ]
    }
   ],
   "source": [
    "model5 = build_model2(lr = 1e-3, lr_d = 1e-7, units = 64, spatial_dr = 0.3, kernel_size1=3, kernel_size2=3, dense_units=64, dr=0.4, conv_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "df0cf40fabd0e970417dd405fcbec2b6812cacb8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "e4440cb6fad4ec9785dbecbafca8ab0b35845b08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66292/66292 [==============================] - 4s 57us/step\n",
      "66292/66292 [==============================] - 5s 68us/step\n",
      "66292/66292 [==============================] - 4s 57us/step\n",
      "66292/66292 [==============================] - 4s 60us/step\n",
      "66292/66292 [==============================] - 4s 60us/step\n"
     ]
    }
   ],
   "source": [
    "pred1 = model1.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred = pred1\n",
    "pred2 = model2.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred += pred2\n",
    "pred3 = model3.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred += pred3\n",
    "pred4 = model4.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred += pred4\n",
    "pred5 = model5.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred += pred5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "6c719d98c75f2391d8489b7c13f93afab71a8fff"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
